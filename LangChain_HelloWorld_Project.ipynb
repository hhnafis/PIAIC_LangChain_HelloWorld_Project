{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMTD47m3AMX/Ty3C2p+UyTD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hhnafis/PIAIC_LangChain_HelloWorld_Project/blob/main/LangChain_HelloWorld_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Project 1: LangChain Hello World Project**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "tntpsHEmBOV9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Description:\n",
        "##In this Project, we will create a simple LangChain Colab Notebook that uses the *Google Gemini Flash 2.0* model to answer user questions."
      ],
      "metadata": {
        "id": "1GgjCkPUBgap"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Install the necessary requirements"
      ],
      "metadata": {
        "id": "U3v6euH5DAK4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "WBR1R1c0BCGY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54868e34-b23b-4588-a4b2-5b5cc54406d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/41.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "#langchain\n",
        "!pip install -Uq langchain\n",
        "\n",
        "#langchain google-genai SDK\n",
        "!pip install -Uq langchain-google-genai"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Project Environment"
      ],
      "metadata": {
        "id": "Sqdt4KyIDLKT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain"
      ],
      "metadata": {
        "id": "V27DIKehDP7T"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Integration of Google_API_Key and configrating Gemini Flash Model"
      ],
      "metadata": {
        "id": "roFQT0NrDutH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    api_key=GOOGLE_API_KEY,\n",
        "    model='gemini-1.5-flash',\n",
        "    temperature=0.2\n",
        ")"
      ],
      "metadata": {
        "id": "0P59JlMPD66v"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Prompt Template"
      ],
      "metadata": {
        "id": "n7UElMJSFoHP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template = PromptTemplate(\n",
        "    input_variables=[\"question\"],\n",
        "    template=\"You are a helpful assistant. Answer the following question:\\n\\n{question}\"\n",
        ")"
      ],
      "metadata": {
        "id": "ZxInmEZeFxAS"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Build the LangChain Pipeline"
      ],
      "metadata": {
        "id": "7nuADeowGwMQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chain = LLMChain(llm=llm, prompt=prompt_template)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "LSX4lDzyG1_T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f1c267d-3a46-41b7-b8a4-355c6d026d7a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-4f9708b53e60>:1: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
            "  chain = LLMChain(llm=llm, prompt=prompt_template)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Run the Hello World Example\n"
      ],
      "metadata": {
        "id": "zOK2Iw6yHMSR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"What is Langchain?\"\n",
        "\n",
        "response = chain.run({\"question\": question})\n",
        "print(\"Answer:\",response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rGUUGOCHHOQA",
        "outputId": "db745b00-73b8-40a5-b26c-aad833a4fdbc"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-6e32f370a4fd>:3: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  response = chain.run({\"question\": question})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: LangChain is a framework for developing applications powered by large language models (LLMs).  It simplifies the process of building applications by providing tools and components for:\n",
            "\n",
            "* **Connecting to LLMs:**  LangChain easily integrates with various LLMs, allowing you to switch between providers (like OpenAI, Hugging Face Hub, etc.) without significant code changes.\n",
            "\n",
            "* **Managing prompts and chains:** It offers structures for organizing prompts and chaining multiple LLMs together to perform complex tasks.  This allows for more sophisticated workflows than simply sending a single prompt to a single LLM.\n",
            "\n",
            "* **Memory management:**  LangChain provides mechanisms for LLMs to retain information across multiple interactions, enabling conversational applications and maintaining context over time.\n",
            "\n",
            "* **Data connection:** It facilitates connecting LLMs to external data sources, such as databases, documents, and APIs, allowing the LLM to access and process information beyond its initial training data.\n",
            "\n",
            "* **Agent capabilities:** LangChain supports building agents that can autonomously choose which LLMs or tools to use based on the task at hand.\n",
            "\n",
            "In essence, LangChain acts as a powerful glue that connects LLMs to other components, making it easier to build robust and sophisticated applications that leverage the capabilities of LLMs effectively.  It abstracts away much of the complexity involved in working with LLMs directly, allowing developers to focus on the application logic rather than the intricacies of LLM interaction.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Complete Script"
      ],
      "metadata": {
        "id": "hSvbcq7uJmh0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#set up the environment\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "#integrate the google api key\n",
        "from google.colab import userdata\n",
        "GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "#configure the Gemini Flash model\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    api_key=GOOGLE_API_KEY,\n",
        "    model='gemini-1.5-flash',\n",
        "    temperature=0.5\n",
        ")\n",
        "\n",
        "#create a prompt template\n",
        "prompt_template = PromptTemplate(\n",
        "    input_variables=[\"question\"],\n",
        "    template=\"You are a helpful assistant. Answer the following question:\\n\\n{question}\"\n",
        ")\n",
        "\n",
        "#buil langchain pipline\n",
        "chain = LLMChain(llm=llm, prompt=prompt_template)\n",
        "\n",
        "#example to test\n",
        "question = \"What is LangGraph?\"\n",
        "\n",
        "response = chain.run({\"question\": question})\n",
        "print(\"Answer:\",response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WtPYKo9iJrZS",
        "outputId": "fffd4999-a9dd-4a8f-f207-f69f342597c5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: LangGraph is a large-scale multilingual knowledge graph constructed by combining multiple knowledge bases and leveraging large language models (LLMs).  It aims to represent knowledge across different languages and domains in a structured and interconnected way.  Essentially, it's a vast network of interconnected concepts and entities, with relationships between them expressed in multiple languages.  This allows for cross-lingual knowledge retrieval and reasoning.  The exact details of its construction and specific features might vary depending on the implementation, as different research groups may build their own versions of a \"LangGraph.\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Next Steps in Colab Project"
      ],
      "metadata": {
        "id": "zEbuFznFLaUA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1-Experiment with Multiple Prompts"
      ],
      "metadata": {
        "id": "UKhrkcLKLc7s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema import HumanMessage, SystemMessage\n",
        "\n",
        "chat = ChatGoogleGenerativeAI(\n",
        "    model='gemini-1.5-flash',\n",
        "    api_key=GOOGLE_API_KEY,\n",
        "    temperature=0.5\n",
        ")\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(content=\"You are a helpful assistant. Answer every question you are being asked.\"),\n",
        "    HumanMessage(content=\"What is Langchain?\"),\n",
        "    HumanMessage(content=\"What is are the key highlights of langchain?\"),\n",
        "    HumanMessage(content=\"What is the capital of Itlay?\")\n",
        "]\n",
        "\n",
        "for message in messages[1:]:\n",
        "    response = chat([messages[0], message])\n",
        "    print(f\"Question: {message.content}\")\n",
        "    print(f\"Answer: {response.content}\\n\")\n",
        "\n",
        "# response = chat(messages)\n",
        "# print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9VvLKnmCLhkx",
        "outputId": "e8c10212-8e32-46b1-fbe3-26f7d5132465"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: What is Langchain?\n",
            "Answer: LangChain is a framework for developing applications powered by language models.  It simplifies the process of building applications that utilize LLMs by providing tools and components for:\n",
            "\n",
            "* **Connecting to various LLMs:** LangChain allows you to easily switch between different language models (like OpenAI's GPT, Hugging Face models, etc.) without changing much of your code.\n",
            "\n",
            "* **Managing chains of calls:**  It lets you create sequences of calls to LLMs, where the output of one call becomes the input for the next.  This is crucial for complex tasks that require multiple steps.  These sequences are called \"chains.\"\n",
            "\n",
            "* **Memory management:**  LangChain provides mechanisms for LLMs to remember previous interactions within a conversation or across multiple calls. This is essential for maintaining context and building more engaging and coherent applications.\n",
            "\n",
            "* **Agent capabilities:**  It enables the development of agents that can interact with their environment (e.g., search the web, access databases) to gather information and complete tasks.  These agents can decide which LLMs or tools to use based on the task at hand.\n",
            "\n",
            "* **Data connection:** LangChain facilitates connecting LLMs to your own data, allowing you to build applications that leverage your specific information.  This might involve connecting to databases, documents, or other data sources.\n",
            "\n",
            "In essence, LangChain acts as a powerful toolkit for building sophisticated applications that go beyond simple prompt-response interactions with LLMs. It offers modularity, flexibility, and ease of use, making it a popular choice for developers working with large language models.\n",
            "\n",
            "Question: What is are the key highlights of langchain?\n",
            "Answer: LangChain's key highlights revolve around its ability to simplify the development of applications powered by large language models (LLMs).  Here are some of the most important:\n",
            "\n",
            "* **Modular Design:** LangChain is built with a modular architecture, allowing developers to easily swap out different components (LLMs, prompts, memory, etc.) to customize their applications. This promotes flexibility and experimentation.\n",
            "\n",
            "* **Abstraction of LLMs:**  It abstracts away the complexities of interacting with various LLMs, allowing you to switch between providers (OpenAI, Hugging Face, etc.) with minimal code changes.\n",
            "\n",
            "* **Chain Functionality:**  LangChain's core strength lies in its ability to chain together multiple operations, creating complex workflows.  This allows for applications beyond simple single-prompt interactions, enabling things like multi-step reasoning, external data retrieval, and more.\n",
            "\n",
            "* **Memory Management:** LangChain provides tools for managing the context and memory of conversations, enabling more coherent and natural interactions with LLMs over time.  This is crucial for applications requiring sustained dialogue or remembering previous interactions.\n",
            "\n",
            "* **Agent Capabilities:**  LangChain facilitates the creation of agents that can interact with their environment (e.g., searching the web, accessing databases) to answer questions and complete tasks.  This allows for more powerful and autonomous applications.\n",
            "\n",
            "* **Data Connection:** LangChain offers tools to connect LLMs to your own data sources. This allows you to build applications that leverage your specific data, rather than relying solely on the LLM's general knowledge.\n",
            "\n",
            "* **Active Community and Documentation:** LangChain has a vibrant and growing community, offering ample support and resources for developers.  The documentation is generally well-maintained and helpful.\n",
            "\n",
            "* **Extensibility:**  The framework is designed to be easily extended, allowing developers to integrate their own custom components and functionalities.\n",
            "\n",
            "In short, LangChain simplifies the process of building sophisticated and powerful applications using LLMs by providing a structured and modular framework, handling much of the underlying complexity.  It empowers developers to focus on the application logic rather than the intricacies of LLM interaction.\n",
            "\n",
            "Question: What is the capital of Itlay?\n",
            "Answer: The capital of Italy is Rome.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2-Add Memory\n",
        "##Use LangChain’s memory feature to make the interaction multi-turn"
      ],
      "metadata": {
        "id": "8CNv-efzZTdc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import  ConversationBufferMemory\n",
        "from langchain.chains import ConversationChain\n",
        "\n",
        "#Initializing the Language Model\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    api_key=GOOGLE_API_KEY,\n",
        "    model=\"gemini-1.5-flash\",\n",
        "    temperature=0.1\n",
        ")\n",
        "#Setting up the Memory\n",
        "memory = ConversationBufferMemory()\n",
        "\n",
        "#Creating the Conversation Chain\n",
        "conversation = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory=memory,\n",
        "    #verbose=True\n",
        ")\n",
        "\n",
        "\n",
        "#Starting the Conversation\n",
        "# ... (previous code) ...\n",
        "\n",
        "#Starting the Conversation\n",
        "print(f\"User: Hi there! My name is Hamza Hanif\")\n",
        "print(f\"Chatbot: {conversation.predict(input='Hi there! My name is Hamza Hanif')}\")\n",
        "print(f\"User: What is the capital of Pakistan?\")\n",
        "print(f\"Chatbot: {conversation.predict(input='What is the capital of Pakistan?')}\")\n",
        "print(f\"User: What we talked about before my name?\")\n",
        "print(f\"Chatbot: {conversation.predict(input='What we talked about before my name?')}\")\n",
        "print(f\"User: What is my name?\")\n",
        "print(f\"Chatbot: {conversation.predict(input='What is my name?')}\")\n",
        "print(f\"User: What we talked about after my name??\")\n",
        "print(f\"Chatbot: {conversation.predict(input='What we talked about after my name??')}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mXJUPgaMZTAm",
        "outputId": "9527a414-ffcb-4b53-ba2c-bb8cb08ebb89"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-041108f9d57f>:11: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory = ConversationBufferMemory()\n",
            "<ipython-input-10-041108f9d57f>:14: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use :meth:`~RunnableWithMessageHistory: https://python.langchain.com/v0.2/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html` instead.\n",
            "  conversation = ConversationChain(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User: Hi there! My name is Hamza Hanif\n",
            "Chatbot: Hi Hamza Hanif! It's nice to meet you.  My name isn't really a \"name\" in the human sense, as I don't have a personal identity like you do.  I'm a large language model, trained by Google.  I don't have feelings or experiences, but I can access and process information from a massive dataset of text and code.  Think of me as a really advanced search engine that can also hold a conversation and generate different creative text formats.  So, what can I do for you today?  Are you interested in learning something new, writing a story, getting help with a task, or just chatting?\n",
            "User: What is the capital of Pakistan?\n",
            "Chatbot: The capital of Pakistan is Islamabad.  It's a planned city, established in 1960, and serves as the country's administrative center.  While Karachi is the largest city in Pakistan, and Lahore holds significant historical and cultural importance, Islamabad is the seat of the federal government and houses key institutions like the Parliament, the President's House, and the Supreme Court.  Is there anything else I can help you with today?\n",
            "User: What we talked about before my name?\n",
            "Chatbot: Before you told me your name, Hamza Hanif, we hadn't spoken at all.  Our conversation began with your introduction.  I had no knowledge of you or any prior context before that point.  Is there anything else I can assist you with?\n",
            "User: What is my name?\n",
            "Chatbot: Your name is Hamza Hanif. You told me that at the beginning of our conversation.\n",
            "User: What we talked about after my name??\n",
            "Chatbot: After you told me your name, Hamza Hanif, we discussed the capital of Pakistan.  I informed you that the capital is Islamabad, and provided some additional details about its history and significance as the administrative center of the country, contrasting it with Karachi and Lahore.  Following that, you asked me what we had talked about before your name, to which I responded that we hadn't spoken at all prior to your introduction.  Then you asked me your name again, and I confirmed it was Hamza Hanif.  And now you've asked me what we talked about after telling me your name, which brings us to this point in our conversation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Integrate Tools\n",
        "##Extend the chain to include tools like database queries or APIs"
      ],
      "metadata": {
        "id": "jJaRBIy94eM7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -Uq langchain-community # Install the missing package"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZcZZOoRJ5uxj",
        "outputId": "1e555e33-ddc4-47d0-dcd7-38438e46a5e8"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.6/49.6 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Note:\n",
        "####In this program I uses the LangChain framework and the Google Gemini Flash 1.5 model to interact with the OpenWeatherMap API to provide weather information for a given city. It takes the city name as input from the user, constructs a prompt asking for the weather in that city, and sends it to the Gemini model. The model's response is printed, and then the program makes a direct call to the OpenWeatherMap API to fetch and display the actual weather data for comparison. Essentially, it's showcasing how to integrate an external API (OpenWeatherMap) [OpenWeatherMap](https://https://openweathermap.org/) into a language model workflow using LangChain."
      ],
      "metadata": {
        "id": "BivGPMT2Ewko"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "# Replace with your actual OpenWeatherMap API key\n",
        "api_key = \"ca558b1e9a7c00654bc7218eeefc1ac7\"\n",
        "\n",
        "def get_weather(city):\n",
        "    \"\"\"Fetches weather data from OpenWeatherMap API.\"\"\"\n",
        "    url = f\"http://api.openweathermap.org/data/2.5/weather?q={city}&appid={api_key}&units=metric\"\n",
        "    response = requests.get(url)\n",
        "    data = response.json()\n",
        "    return f\"The weather in {city} is currently {data['weather'][0]['description']}. Temperature: {data['main']['temp']}°C\"\n",
        "\n",
        "# Initialize the LLM\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    api_key=GOOGLE_API_KEY,\n",
        "    model=\"gemini-1.5-flash\",\n",
        "    )\n",
        "\n",
        "# Define the prompt template\n",
        "prompt_template = \"\"\"\n",
        "What is the weather like in {city}?\n",
        "\"\"\"\n",
        "prompt = PromptTemplate(\n",
        "    template=prompt_template, input_variables=[\"city\"]\n",
        ")\n",
        "\n",
        "# Create an LLMChain\n",
        "chain = LLMChain(prompt=prompt, llm=llm)\n",
        "\n",
        "# Get user input for the city\n",
        "city = input(\"Enter the city: \")\n",
        "\n",
        "# Generate response from the LLM\n",
        "weather_info = chain.run(city=city)\n",
        "print(weather_info)\n",
        "\n",
        "# Get actual weather data from OpenWeatherMap API\n",
        "city_weather = get_weather(city)\n",
        "print(city_weather)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YUPQGUrI__D1",
        "outputId": "bfaeea5c-ce56-4021-e7e2-4599f3918379"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the city: karachi\n",
            "I do not have access to real-time information, including current weather conditions.  To get the current weather in Karachi, Pakistan, I recommend checking a reliable weather website or app such as:\n",
            "\n",
            "* **Google Weather:** Simply search \"weather Karachi\" on Google.\n",
            "* **AccuWeather:**  Search for Karachi on their website or app.\n",
            "* **Weather.com (The Weather Channel):**  Look up Karachi on their website or app.\n",
            "\n",
            "These sources will give you the most up-to-date information on temperature, humidity, precipitation, and wind conditions.\n",
            "The weather in karachi is currently haze. Temperature: 19.9°C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#LangChain-powered AI Colab Notebook Project with Gemini Flash!\n",
        "###Ended here!"
      ],
      "metadata": {
        "id": "NJhW9oknFl_n"
      }
    }
  ]
}